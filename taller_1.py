# -*- coding: utf-8 -*-
"""Taller 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zW7-QKO8KlCn-Jf4JgngjZjhVeVt4tsg

**Redes Neuronales - Funcionalidades taller Uno.**


El Primer paso es importar las librerias y cargar el modelo que se utilizará, para este caso en concreto utilizaremos un modelo pre-entrenado de TensorFlow.
"""

import tensorflow as tf             #Biblioteca de tensorflow
import tensorflow_datasets as tfds  #Datasets de entrenamiento

from urllib import parse
from http.server import HTTPServer, BaseHTTPRequestHandler

import math
import numpy as np
import matplotlib.pyplot as plt
import logging

logger = tf.get_logger()

logger.setLevel(logging.ERROR)

"""Para entrenar a la inteligencia artificial usaremos el dataset de mnist-fashion para reconocer prendas de vestir 

"""

data, metadata = tfds.load('fashion_mnist', as_supervised = True, with_info = True)

"""Guardamos todos los datos de entrenamiento y pruebas en dos variables distintas."""

test_data, fit_data = data['test'], data['train']

"""Las Imagenes se deben normalizar, esto para que el modelo de inteligencia artificial sea capaz de almacenar cada pixel de la imagen en una neurona para poder procesarlo. Se define una funcion para que cumpla esta tarea.

Tambien se guarda en una variable todas las clasificaciones de las prendas de vestir, esto con el objetivo de mostrar al usuario a través de una interfaz si la IA se equivoco en la prediccion o si esta en lo correcto.
"""

clases = metadata.features['label'].names

def norm(images, labels):
    images = tf.cast(images, tf.float32)
    images /= 255                   
    return images, labels

#Una vez definida la función, los datos de entrenamiento y de prueba deben ser normalizados
test_data = test_data.map(norm)
fit_data = fit_data.map(norm)

#Agregamos en cache los datos para que el proceso de entrenamiento sea mas rapido
test_data = test_data.cache()
fit_data = fit_data.cache()

"""Se define el modelo con un tipo de capa plano, dos tipo de capa densas y la capa de salida para interpretar la respuesta de la IA."""

modelo = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28,1)),       # La capa de entrada son de unas 784, cada una por el pixel de la imagen de las prendas de vestir
    tf.keras.layers.Dense(50, activation=tf.nn.relu),     # Luego se utilizan 2 capas densas con las funciones de activación relu, esto hara que el modelo de IA sea capaz de tener mas opciones
    tf.keras.layers.Dense(50, activation=tf.nn.relu),     # a la hora de clasificar las prendas de vestir.
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)   # Por último una capa de salida de 10 neuronas, esto por cada tipo de prenda, la suma de estas neuronas nos dara siempre uno.
])

#se compila el modelo

modelo.compile(
    optimizer = 'adam',
    loss = tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy']
)

"""Establecemos una variable con la cantidad de lotes que usara el modelo para entrenar. Para hacer mas aleatorio el entrenamiento, los datos de entrenamiento (fit_data) deben estar aleatorizados, para aquello utilizamos la funcion '.shuffle' y tambien deben repetirse con la función '.repeat'."""

s_size = 32

fit_data = fit_data.repeat().shuffle(60000).batch(s_size)
test_data = test_data.batch(s_size)

"""Una vez construido el modelo, sera capaz de entrenarlo con la función .fit, esta función permite a la inteligencia artificial clasificar imágenes (en este caso desde un dataset de prendas de vestir) para que luego el usuario pueda testearla mostrandole imagenes y viendo si las predicciones son correctas o erróneas."""

historial = modelo.fit(fit_data, epochs=10, steps_per_epoch= math.ceil(60000/s_size))


"""Para efectos del taller se requiere un frontend para la interaccion y testeo del modelo a tráves de una interfaz."""

#Exportacion del modelo a h5
modelo.save('modelo_exportado.h5')

"""Para crear el modelo lo que se planteo fue lo siguiente: 
1.- Dentro de la terminal de python se creo una carpeta de nombre 'modelo_sep' "mkdir modelo_sep"
2.- Se instala tensorflowjs, el cual tiene el convertidor para exportar el modelo en un archivo .js, "pip install tensorflowjs"
3.- Se convierte el archivo .h5 en un .json con el siguiente comando "tensorflowjs_converter --input_format keras modelo_exportado.h5 modelo_sep"
"""